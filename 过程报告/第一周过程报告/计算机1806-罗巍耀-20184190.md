<div align='center' ><h1 style="text-align:center">第一次过程报告 </h1></div>

<p align="center">计1806-罗巍耀-20184190<br></p>

# 一、前期课设动员与准备（6.26-6.28）
## 1、明确课设目标
  * 经过动员会和小组内讨论，我们决定课设完成整个编译器的设计与开发
## 2、任务分工
      - 由于符号贯穿整个编译的前端和后端，于是在前期全组共同完成符号表的设计
      - 丁子恒、高一峰完成词法分析部分
      - 安凯凯、罗巍耀完成语义分析以及中间代码生成部分
      - 张钊源实现语法分析器的基本框架
  * 由于语义分析以及中间代码生成部分涉及的内容较多，在编译课程的学习过程中也觉得较为困难，如果其他部分有先完成的可以支援语义部分
# 二、第一、二次上机（6.29-7.1）
## 1、小组合作部分
  * 符号表是标识符的动态语义词典，属于编译中语义分析的知识库，主要包括：
      - 名字
      - 类型
      - 种类
      - 地址
  * 包括符号表总表`SYNBL`，类型表`TAPEL`，数组表`AINFL` ，结构表`RINFL` ，函数表`PFINFL`和其他表
  * 由于我们采用的是带类的C语言，符号表组织部分为了表示更加高效，采用STL库来作为存储数据的方式
## 2、个人工作部分
  * Github学习部分
很早以前就听说过Github这个平台，正好利用这次编译课设的机会对Github有了更为深入的了解。<br>
      - 通过研究生学长学姐以及老师们的讲解，对Github的基本操作有了基本了解
      - 学会如何Push和Pull，发表Issues；并将本地的项目和Github上的项目建立联系
  * 理清语法-语义-中间代码之间的思路
      - 语法部分输入的是词法分析器产生的token串流，通过语法分析，能够自动生成first集，follow集，select集，进而构建出LL1分析表
      - 语义分析本质上就是生成四元式，根据输入的语义动作进行语法制导翻译，输入为前部分语法分析中自动生成的LL1分析表
      - 将其中包含的语义动作当作文法右部的普通变元来处理，逆序压栈，当遇到语义动作的非终结符时，执行相应的语义动作，同时将生成的四元式输出到vector<quat>中
      - 文法的设计。这里的文法时LL1分析法所用的文法，根据输入的Token串流，生成first集，follow集，select集以及根据输入的语义动作完成四元式的生成
俗话说的好：工欲善其事必先利其器。第一、二次上机时理清脉络，能为后面的项目进展打下坚实的基础，
# 三、第三、四次上机（7.2-7.4）
## 1、小组合作部分
  * 在与其他组员的沟通过程中发现了问题，究竟何时能够判断使用LL1文法以及何时能判断使用递归下降，于是针对这个问题，我们小组请教了张俐老师，得到的结果是文法的使用
并没有严格的界限，我们可以在刚开始分析时采用LL1分析法，当分析到语句时就改用递归下降的方法。
## 2、个人工作部分
  * 此时，前期工作的词法分析部分已经完成，符号表也早已确立，我随机开始了代码编写部分，目的是产生四元式。

```cpp
void Parser::Get_Quats() {
    stack<int> S;
    stack<Action> SACT;
    S.push(gram.Start);
    int d = 0;
    while(!S.empty()) {
        int i = S.top(), j = tokens[d].Vt_id;
//        printf("level = %d\n", tokens[d].level);
//        Debug_Quats(S, SACT, j, d);
        if(i == -1) {   //action is the top
            S.pop();
            if(SACT.empty()) { puts("SACT is null!"); exit(0); }
            Action act = SACT.top(); SACT.pop();
//            printf("act = %s\n", acttype[act.type-1].c_str());
//            printf("act = %d\n", act.type);
            if(act.type == 1) {     //GEQ
                GEQ(act.object, act.tokenid);
            }
            else if(act.type == 2) {
                PUSH(act.object, act.tokenid);
            }
            else if(act.type == 3) {
                ASSI();
            }
            else if(act.type == 4) {
                IF();
            }
            else if(act.type == 5) {
                ELSE();
            }
            else if(act.type == 6) {
                ENDIF();
            }
            else if(act.type == 7) {
                LAB();
            }
            else if(act.type == 8) {
                GOTO();
            }
            else if(act.type == 9) {
                WE();
            }
            else if(act.type == 10) {
                WH();
            }
            else if(act.type == 11) {
                DO();
            }
        }
        else if(gram.Vt.count(i)) {
            if(S.top() == j) {
                d++;
                S.pop();
            }
        }
        else if(gram.Vn.count(i)) {
            S.pop();
            if(LL1Table.count({i, j})) {
                vector<int> rev;
                int k = 0;      //count the position
                int t = LL1Table[{i, j}];   //the index of product
                for(auto x : gram.Gram[t].se) {
                    if(gram.names[x] == "~") continue;
                    rev.pb(x);
                    if(acts[t].type == 0) continue;
                    if(k == acts[t].pos) {
                        rev.pb(-1);
                        int r = tokens[d].i, c = tokens[d].j;
                        acts[t].object = lex.Table[r][c];
                        acts[t].tokenid = d;
//                        Print_Act(acts[t]);
//                        printf("t = %d\n", t);
                        SACT.push(acts[t]);
                    }
                    ++k;
                }
                reverse(rev.begin(), rev.end());
                for(auto x : rev) S.push(x);
            }
        }
    }
}
```
